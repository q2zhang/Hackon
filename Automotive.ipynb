{"cells":[{"cell_type":"markdown","source":["#Customer Segmentation Using MLlib ALS Algorithm"],"metadata":{}},{"cell_type":"code","source":["dbutils.fs.help()"],"metadata":{},"outputs":[],"execution_count":2},{"cell_type":"code","source":["automobile_category_df = spark.read.json(\"/FileStore/tables/cnfldt2b1504923605351/meta_Automotive_json-31ec7.gz\")\ndisplay(automobile_category_df)"],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"code","source":["automobile_df = spark.read.json(\"/FileStore/tables/0cpczmu11504368060244/reviews_Automotive_5_json-afd9e.gz\")\nprint automobile_df.dtypes\nautomobile_df.show(2)\nautomobile_df.printSchema()"],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"code","source":["display(automobile_df.describe())"],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"code","source":["review_df = automobile_df.select('asin', 'helpful', 'overall', 'reviewerID', 'reviewText', 'unixReviewTime')\nreview_df = review_df.withColumnRenamed('overall', 'rating').withColumnRenamed('asin', 'product').withColumnRenamed('reviewerId', 'user').sort('unixReviewTime', ascending=False) \nreview_df.show(2)"],"metadata":{},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":["## Visualization"],"metadata":{}},{"cell_type":"code","source":["display(review_df.select('rating'))"],"metadata":{},"outputs":[],"execution_count":8},{"cell_type":"code","source":["display(review_df.select('rating', 'unixReviewTime'))"],"metadata":{},"outputs":[],"execution_count":9},{"cell_type":"code","source":["display(review_df.select('unixReviewTime'))"],"metadata":{},"outputs":[],"execution_count":10},{"cell_type":"markdown","source":["##data preparation"],"metadata":{}},{"cell_type":"code","source":["from pyspark.ml import Pipeline\nfrom pyspark.ml.feature import StringIndexer\nreview_df = review_df.withColumn('timeStamp', review_df.unixReviewTime.cast(\"timestamp\").cast(\"date\"))\nindexers = [StringIndexer(inputCol=column, outputCol=column+\"Id\").fit(review_df) for column in ['product', 'user'] ]\npipeline = Pipeline(stages=indexers)\nindexed_df = pipeline.fit(review_df).transform(review_df)\nindexed_df = (indexed_df.withColumn(\"productId\", indexed_df[\"productId\"].cast('Int'))\n                       .withColumn(\"userId\", indexed_df[\"userId\"].cast('Int'))) "],"metadata":{},"outputs":[],"execution_count":12},{"cell_type":"code","source":["# review summary\nreview_count = indexed_df.count()\nprod_count = indexed_df.select('product').distinct().count()\nuser_count = indexed_df.select('user').distinct().count()\navg_num_review_prod = review_count/prod_count\nprint 'There are %s reviews and %s products in the datasets.  there are average %s reviews per product from %s users.' % (review_count, prod_count, avg_num_review_prod, user_count )"],"metadata":{},"outputs":[],"execution_count":13},{"cell_type":"code","source":["from pyspark.sql import functions as F\nprod_avg_ratings_df = indexed_df.groupBy('productId').agg(F.count(indexed_df.rating).alias('count'), F.avg(indexed_df.rating).alias('average')).sort('average', 'count', ascending=False)    \nprod_avg_ratings_df.show(3)"],"metadata":{},"outputs":[],"execution_count":14},{"cell_type":"code","source":["display(prod_avg_ratings_df)"],"metadata":{},"outputs":[],"execution_count":15},{"cell_type":"code","source":["user_avg_ratings_df = indexed_df.groupBy('userId').agg(F.count(indexed_df.rating).alias('user_count'),F.avg(indexed_df.rating).alias('user_average')).sort('user_average', 'user_count', ascending=False) \nuser_avg_ratings_df.show(3)"],"metadata":{},"outputs":[],"execution_count":16},{"cell_type":"code","source":["display(user_avg_ratings_df)"],"metadata":{},"outputs":[],"execution_count":17},{"cell_type":"code","source":["'''\nmost ratings are above 4.5, How to deal with highly skewed towards very positive reviews?\n# 1. Add an adjustment factor by a helpful score: percentage of helpful\n'''\nfrom pyspark.sql.functions import when\nepsilon = 1e-16\nadj = 1.0\nmin_helpful_vote = 2.0\n\nadjusted_df = indexed_df.withColumn('adjusted_rating', when( (indexed_df['rating'] >= 4.0) & (indexed_df.helpful[1] >= min_helpful_vote), indexed_df.rating +  adj*(indexed_df.helpful[0]/indexed_df.helpful[1].cast(\"Double\"))).otherwise( indexed_df.rating -  adj*(indexed_df.helpful[0]/(indexed_df.helpful[1].cast(\"Double\") + epsilon ))))\n\ndisplay(adjusted_df)"],"metadata":{},"outputs":[],"execution_count":18},{"cell_type":"code","source":["# scale the ratings to 5\nmax_val = adjusted_df.agg({\"adjusted_rating\": \"max\"}).collect()[0][0]\nmin_val = adjusted_df.agg({\"adjusted_rating\": \"min\"}).collect()[0][0]\nscaled_df = adjusted_df.withColumn(\"scaled_rating\", 5*(adjusted_df.adjusted_rating - min_val)/(max_val-min_val)).drop('adjusted_rating')\ndisplay(scaled_df.select('scaled_rating'))"],"metadata":{},"outputs":[],"execution_count":19},{"cell_type":"code","source":["# average rating and number of rating each product received computed based on adjusted scaled dataset\nprod_avg_ratings_scaled_df = scaled_df.groupBy('productId').agg(F.count(scaled_df.scaled_rating).alias('count'), F.avg(scaled_df.scaled_rating).alias('average')).sort('average', 'count', ascending=False)    \nprod_avg_ratings_scaled_df.show(3)"],"metadata":{},"outputs":[],"execution_count":20},{"cell_type":"code","source":["display(prod_avg_ratings_scaled_df)"],"metadata":{},"outputs":[],"execution_count":21},{"cell_type":"code","source":["# average rating and number of rating given by each user computed based on adjusted scaled dataset\nuser_avg_ratings_scaled_df = scaled_df.groupBy('userId').agg(F.count(scaled_df.scaled_rating).alias('user_count'),F.avg(scaled_df.scaled_rating).alias('user_average')).sort('user_average', 'user_count', ascending=False) \nuser_avg_ratings_scaled_df.show(3)"],"metadata":{},"outputs":[],"execution_count":22},{"cell_type":"code","source":["display(user_avg_ratings_scaled_df)"],"metadata":{},"outputs":[],"execution_count":23},{"cell_type":"code","source":["display(scaled_df)"],"metadata":{},"outputs":[],"execution_count":24},{"cell_type":"code","source":["'''\n2. Time factor, rating time is a significant factor for automobile. Using algorithm similar to following to adjust the rating score further\nHacker News formula (https://medium.com/hacking-and-gonzo/how-hacker-news-ranking-algorithm-works-1d9b0cf2c08d) :\nScore = (P-1) / (T+2)^G\n\nwhere,\nP = points of an item (and -1 is to negate submitters vote)\nT = time since submission (in hours)\nG = Gravity, defaults to 1.8  \n'''\nimport time\nimport datetime\ndef calculate_score(votes, item_hour_age, gravity=1.8):\n    return (votes - 1) / pow((item_hour_age+2), gravity)\n  \ngravity = 0.01\ncurrent_time = datetime.date.fromtimestamp(time.time())\nscaled_df = scaled_df.withColumn(\"current_time\",  F.lit(current_time))\ntime_adjusted_df =  scaled_df.withColumn('time_adjusted_rating', (scaled_df.rating - 1)/pow((F.datediff(scaled_df['current_time'], scaled_df['timeStamp'])/365+2), gravity)).drop('current_time') \ntime_adjusted_df.show(2)"],"metadata":{},"outputs":[],"execution_count":25},{"cell_type":"code","source":["# scale the ratings again to 5\nmax_val = time_adjusted_df.agg({\"time_adjusted_rating\": \"max\"}).collect()[0][0]\nmin_val = time_adjusted_df.agg({\"time_adjusted_rating\": \"min\"}).collect()[0][0]\nscaled_df = time_adjusted_df.withColumn(\"scaled_time_adjusted_rating\", 5*(time_adjusted_df.time_adjusted_rating - min_val)/(max_val-min_val)).drop('\ttime_adjusted_rating')\ndisplay(scaled_df.select('scaled_time_adjusted_rating'))"],"metadata":{},"outputs":[],"execution_count":26},{"cell_type":"code","source":["# average rating and number of rating each product received computed based on adjusted scaled dataset\nprod_avg_ratings_time_scaled_df = scaled_df.groupBy('productId').agg(F.count(scaled_df.scaled_time_adjusted_rating).alias('count'), F.avg(scaled_df.scaled_time_adjusted_rating).alias('average')).sort('average', 'count', ascending=False)    \ndisplay(prod_avg_ratings_time_scaled_df)"],"metadata":{},"outputs":[],"execution_count":27},{"cell_type":"code","source":["user_avg_ratings_time_scaled_df = scaled_df.groupBy('userId').agg(F.count(scaled_df.scaled_time_adjusted_rating).alias('user_count'),F.avg(scaled_df.scaled_time_adjusted_rating).alias('user_average')).sort('user_average', 'user_count', ascending=False) \ndisplay(user_avg_ratings_time_scaled_df)"],"metadata":{},"outputs":[],"execution_count":28},{"cell_type":"code","source":["# create traing/valication/testing sets based on time:  testing set, 2014-03-12 ~ 2014-07-12; vtalication set: 2013-12-12 ~ 2014-03-11; the rest of the data as training set\n#test_df = scaled_df.where(scaled_df['timeStamp'] > '2014-03-12')\n#validation_df = scaled_df.where((scaled_df['timeStamp'] >= '2013-12-12') & (scaled_df['timeStamp'] <= '2014-03-12'))\n#training_df = scaled_df.where(scaled_df['timeStamp'] < '2013-12-12')\n\nseed = 100009193L\n(split_70_df, split_a_15_df, split_b_15_df) =  scaled_df.randomSplit([0.7, 0.15, 0.15], seed)\ntraining_df = split_70_df.cache()\nvalidation_df = split_a_15_df.cache()\ntest_df = split_b_15_df.cache()\n\nprint('Training: {0}, validation: {1}, test: {2}\\n'.format(\n  training_df.count(), validation_df.count(), test_df.count())\n)\ntraining_df.show(2)\nvalidation_df.show(2)\ntest_df.show(2)"],"metadata":{},"outputs":[],"execution_count":29},{"cell_type":"code","source":["# baseline recommender: highest average rating and recieved number of ratings more than average number of ratings\ntrain_avg_ratings_df = training_df.groupBy('productId').agg(F.count(training_df.scaled_time_adjusted_rating).alias('count'), F.avg(training_df.scaled_time_adjusted_rating).alias('average'))     \ntrain_with_avg_or_more_num_rating = train_avg_ratings_df.where(train_avg_ratings_df['count'] >= avg_num_review_prod).sort('average', 'count', ascending=False)\ntrain_with_avg_or_more_num_rating.show(3)"],"metadata":{},"outputs":[],"execution_count":30},{"cell_type":"code","source":["# RMSE baseline, calculate RMSE on training and test dataset using average rating from train_with_avg_or_more_num_rating\nfrom pyspark.ml.evaluation import RegressionEvaluator\npred_avg = train_with_avg_or_more_num_rating.agg(F.avg(train_with_avg_or_more_num_rating.average).alias('avg'))\ntrain_with_avg_pred_df = training_df.withColumn('avg', F.lit(pred_avg.collect()[0][0]))\nbase_reg_eval = RegressionEvaluator(predictionCol=\"avg\", labelCol=\"rating\", metricName=\"rmse\")\nbase_training_RMSE = base_reg_eval.evaluate(train_with_avg_pred_df)\nprint('The base line RMSE on the training set of {0}'.format(base_training_RMSE))\n\ntest_with_avg_pred_df = test_df.withColumn('avg', F.lit(pred_avg.collect()[0][0]))\nbase_test_RMSE = base_reg_eval.evaluate(test_with_avg_pred_df)\nprint('The base line RMSE on the test set of {0}'.format(base_test_RMSE)) "],"metadata":{},"outputs":[],"execution_count":31},{"cell_type":"code","source":["# Recommendation Using ASL algorithm\nfrom pyspark.ml.recommendation import ALS\n\nseed = 1800009193L\nranks = [4, 8, 12, 16]\nerrors = [[0]*len(ranks)][0]\nmodels = [[0]*len(ranks)][0]\ndef als_model(training_df, validation_df, rating, num_iter):\n  als = ALS()\n  als.setMaxIter(num_iter)\\\n     .setSeed(seed)\\\n     .setRegParam(0.1)\\\n      .setItemCol('productId')\\\n      .setRatingCol(rating)\\\n      .setUserCol('userId')\n      \n  reg_eval = RegressionEvaluator(predictionCol=\"prediction\", labelCol = rating, metricName=\"rmse\")\n\n  tolerance = 0.03\n  err = 0\n  min_error = float('inf')\n  best_rank = -1\n\n  for rank in ranks:\n    # Set the rank here:\n    als.setRank(rank)\n    # Create the model with these parameters.\n    model = als.fit(training_df)\n    # Run the model to create a prediction. Predict against the validation_df.\n    predict_df = model.transform(validation_df).select(\"userId\", \"productId\", rating, \"prediction\")\n\n    # Remove NaN values from prediction (due to SPARK-14489)\n    predicted_ratings_df = predict_df.filter(predict_df.prediction != float('nan'))\n \n    # Run the previously created RMSE evaluator, reg_eval, on the predicted_ratings_df DataFrame\n    error = reg_eval.evaluate(predicted_ratings_df)\n    errors[err] = error\n    models[err] = model\n    print 'For rank %s the RMSE is %s' % (rank, error)\n    if error < min_error:\n      min_error = error\n      best_rank = err\n    err += 1\n\n  als.setRank(ranks[best_rank])\n  print 'The best model was trained with rank %s' % ranks[best_rank]\n  return models[best_rank]"],"metadata":{},"outputs":[],"execution_count":32},{"cell_type":"code","source":["my_model = als_model(training_df, validation_df, 'rating', 15)"],"metadata":{},"outputs":[],"execution_count":33},{"cell_type":"code","source":["model_rank_rmse = sqlContext.createDataFrame(zip(ranks, errors), [\"Rank\", \"RMSE_rating\"])\ndisplay(model_rank_rmse)"],"metadata":{},"outputs":[],"execution_count":34},{"cell_type":"code","source":["# test\nreg_eval = RegressionEvaluator(predictionCol=\"prediction\", labelCol =\"rating\", metricName=\"rmse\")\npredict_df = my_model.transform(test_df).select(\"userId\", \"productId\", \"rating\", \"prediction\")\npredicted_test_df = predict_df.filter(predict_df.prediction != float('nan'))\ntest_RMSE = reg_eval.evaluate(predicted_test_df)\n\nprint('The model had a RMSE on the test set of {0}'.format(test_RMSE)) "],"metadata":{},"outputs":[],"execution_count":35},{"cell_type":"code","source":["my_model = als_model(training_df, validation_df, 'scaled_rating', 15)"],"metadata":{},"outputs":[],"execution_count":36},{"cell_type":"code","source":["model_rank_rmse = sqlContext.createDataFrame(zip(ranks, errors), [\"Rank\", \"RMSE_scaled_rating\"])\ndisplay(model_rank_rmse)"],"metadata":{},"outputs":[],"execution_count":37},{"cell_type":"code","source":["# test\nreg_eval = RegressionEvaluator(predictionCol=\"prediction\", labelCol =\"scaled_rating\", metricName=\"rmse\")\npredict_df = my_model.transform(test_df).select(\"userId\", \"productId\", \"scaled_rating\", \"prediction\")\npredicted_test_df = predict_df.filter(predict_df.prediction != float('nan'))\ntest_RMSE = reg_eval.evaluate(predicted_test_df)\n\nprint('The model had a RMSE on the test set of {0}'.format(test_RMSE)) "],"metadata":{},"outputs":[],"execution_count":38},{"cell_type":"code","source":["my_model = als_model(training_df, validation_df, 'scaled_time_adjusted_rating', 15)"],"metadata":{},"outputs":[],"execution_count":39},{"cell_type":"code","source":["model_rank_rmse = sqlContext.createDataFrame(zip(ranks, errors), [\"Rank\", \"RMSE_scaled_time_adjusted_rating\"])\ndisplay(model_rank_rmse)"],"metadata":{},"outputs":[],"execution_count":40},{"cell_type":"code","source":["# test\nreg_eval = RegressionEvaluator(predictionCol=\"prediction\", labelCol =\"scaled_time_adjusted_rating\", metricName=\"rmse\")\npredict_df = my_model.transform(test_df).select(\"userId\", \"productId\", \"scaled_time_adjusted_rating\", \"prediction\")\npredicted_test_df = predict_df.filter(predict_df.prediction != float('nan'))\ntest_RMSE = reg_eval.evaluate(predicted_test_df)\n\nprint('The model had a RMSE on the test set of {0}'.format(test_RMSE)) "],"metadata":{},"outputs":[],"execution_count":41},{"cell_type":"code","source":["userRecs = my_model.recommendForAllUsers(5).withColumnRenamed(\"userId\", \"Id\")\nrecommend_prod_df = test_df.select(\"userId\").join(userRecs, test_df.userId == userRecs.Id).drop(\"Id\")\nprint recommend_prod_df.count()\ndisplay(recommend_prod_df)"],"metadata":{},"outputs":[],"execution_count":42},{"cell_type":"code","source":["prodRecs = my_model.recommendForAllItems(50).withColumnRenamed(\"productId\", \"Id\")\nrecommend_user_df = test_df.select(\"productId\").join(prodRecs, test_df.productId == prodRecs.Id).drop(\"Id\")\ndisplay(recommend_user_df)\n"],"metadata":{},"outputs":[],"execution_count":43}],"metadata":{"name":"Automotive","notebookId":1408957679795198},"nbformat":4,"nbformat_minor":0}
